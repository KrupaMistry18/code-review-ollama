# Use Ollama locally
USE_OLLAMA=true

# Pick a model you've pulled (see: `ollama list`)
# Examples: llama3.1:8b-instruct, mistral:7b-instruct, phi3:3.8b
OLLAMA_MODEL=phi3:3.8b

# Default Ollama server
OLLAMA_BASE_URL=http://localhost:11434
